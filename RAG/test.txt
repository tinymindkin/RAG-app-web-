ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos\nHaolin Yang1,3∗, Feilong Tang1,2,3∗, Ming Hu1,2,3, Yulong Li1,3, Junjie 
Guo4,\nYexin Liu5, Zelin Peng6, Junjun He3, Zongyuan Ge2†, Imran Razzak1†\n1MBZUAI, 2Monash University, 3Shanghai AI Lab, 4Nanjing University, 5HKUST, 6Shanghai Jiaotong University,\nProject Page: https://yanghlll.github.io/ScalingNoise.github.io/\n"A cat wearing sun glasses and working as a lifeguard at a pool, 4k resolution."\nChunk by Chunk\nFIFO-Diffusion \n(b) Inference-time Scaling Search for Golden Noises (ScalingNoise)\n(a) Long videos generated by different denoising methods\n+ScalingNoise\n+ScalingNoise\nOne-step \nDenoise\nCandidate\nNoise\n1：\n2：\n(c) Sampling Noise by Reward Model Guided\nReward \nModel\nAnchor\nReward\n0.86\nReward \n0.96\n......\noutput\nnoise\nchunk\nAnchor\nI=1\nI=2\nI=N\nVideo Index\nSelected\nnoise\nRejected\nnoise\n......\nAnchor Frame\nAnchor\nFigure 1. An overview of how ScalingNoise improves long video generation through inference-time search. (a) Chunk-by-chunk and\nFIFO-Diffusion methods often suffer from accumulated errors and visual degradation over long sequences. (b) ScalingNoise mitigates this\nby conducting a tailored step-by-step beam search for suitable initial noises, guided by a reward model that incorporates an anchor frame\nto ensure a long-term signal. (c) At each step, we perform one-step denoising on candidate noises to obtain a clearer clip for evaluation;\nthe reward model then predicts the long-term value of each candidate, helping avoid noises that could introduce future inconsistencies.\nAbstract\nVideo diffusion models (VDMs) facilitate the generation of\nhigh-quality videos, with current research predominantly\nconcentrated on scaling efforts during training through im-\nprovements in data quality, computational resources, and\nmodel complexity. However, inference-time scaling has re-\nceived less attention, with most approaches restricting mod-\nels to a single generation attempt. Recent studies have un-\ncovered the existence of “golden noises” that 
can enhance\nvideo quality during generation. Building on this, we find\nthat guiding the scaling inference-time search of VDMs to\nidentify better noise candidates not only evaluates the qual-\nity of the frames generated in the current step but also pre-\nserves the high-level object features by referencing the an-\nchor frame from previous multi-chunks, thereby delivering\nlong-term value. Our analysis reveals that diffusion mod-\n1Equal Contribution. † Corresponding Authors.\nels inherently possess flexible adjustments of computation\nby varying denoising steps, and even a one-step denoising\napproach, when guided by a reward signal, yields signifi-\ncant long-term benefits. Based on the observation, we pro-\npose ScalingNoise, a plug-and-play inference-time search\nstrategy that identifies golden initial noises for the diffu-\nsion sampling process to improve global content consis-\ntency and visual diversity. Specifically, we perform one-\nstep denoising to convert initial noises into a clip and sub-\nsequently evaluate its long-term value, leveraging a reward\nmodel anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted\nnoise distribution that up-weights promising noises. In this\nway, ScalingNoise significantly reduces noise-induced er-\nrors, ensuring more coherent and spatiotemporally consis-\ntent video generation. Extensive experiments on benchmark\ndatasets demonstrate that the proposed ScalingNoise effec-\ntively improves both content fidelity and subject consistency\nfor resource-constrained long video generation.\n1\narXiv:2503.16400v1  [cs.LG]  20 Mar 2025\n1. Introduction\nLong video generation has a significant impact on various\napplications, including film production, game development,\nand artistic creation [39, 87, 94]. Compared to image gen-\neration [16, 41, 80], video generation demands significantly\ngreater data scale and computational resources due to the\nhigh-dimensional nature of video data. This necessitates a\ntrade-off between limited resources and model performance\nfor Video Diffusion Models (VDMs) [35, 48, 65, 82].\nRecent VDMs typically adopt two main methods: one\nis the chunked autoregressive strategy [4, 23, 26, 43, 72],\nwhich predicts several frames in parallel conditioned on\na few preceding ones, consequently reducing the com-\nputational burden, and “diagonal denoising” from FIFO-\ndiffusion [11, 31], which re-plans the time schedule and\nmaintains a queue with progressively increasing noise lev-\nels for denoising. However, video generation is induced by\nboth the diffusion strategies and the noise. Variations in\nthe noises can lead to substantial 
changes in the synthesized\nvideo, as even minor alterations in the noise input can dra-\nmatically influence the output [51, 89, 115]. This sensitivity\nunderscores that noises affect both the overall content and\nthe subject consistency of long video generation.\nThe key to enhancing the quality of long video gener-\nation lies in identifying “golden noises” for the diffusion\nsampling process. Recent studies employ the approach of\nincreasing 
data [8, 17, 88, 98], computational resources [30,\n83, 97, 106, 108, 111], and model size [20, 37, 44, 49, 114]\nto reduce the truncation errors during the sampling pro-\ncess, but these methods often incur substantial additional\ncosts. Conversely, other approaches focus on training-free\ndenoising strategies such as FreeNoise [42, 53, 81, 115]\nand Gen-L-Video [74]. They aim to enhance the consis-\ntency of generated video by refining local denoising pro-\ncesses to mitigate noise-induced discrepancies, thereby en-\nsuring smoother temporal transitions. Recently, in Large\nLanguage Models (LLMs), the study on improving their ca-\npability has expanded to inference-time [1, 36, 62, 93]. By\nallocating more compute during inference, often 
through\nsophisticated search processes, these works show that\nLLMs can produce higher-quality and more appropriate re-\nsponses. As a result, inference-time scaling opens new av-\nenues for improving model performance when additional\nresources become available after training.\nSimilarly, re-\ncent explorations in diffusion models have leveraged the\nextra inference-time compute to refine noise search and\nenhance denoising, thereby improving sample quality and\nconsistency [45, 50, 75]. However, while previous works\nhave shown effectiveness, they focus solely on information\nwithin a local window, overlooking long-term feedback and\naccumulated errors.\nIn this study, we argue that scaling\ninference-time search of VDMs to identify golden noises\nenhances long-term consistencies in long video generation.\nSearch\nRepresentative Methods\nType\nAdvantage\nGreedy\nChunk-Wise Generation 
[107]\nVideo\nE\nTree\nScaling Denoising Steps [45]\nImage\nS E\nSteering Generation [60]\nImage\nScalingNoise (Ours)\nVideo\nG S E\nTable 1. Greedy decoding is efficient but easily trapped in local\noptima. Tree methods, better for global optimal decisions, is suit-\nable for inference-time scaling. Our ScalingNoise achieves: G\nlobal-Optimality of solution, S caling to long-range planning, and\nE fficiency.\nTo this end, we propose ScalingNoise, a plug-and-play\ninference-time search strategy that identifies golden initial\nnoises by leveraging a reward model to steer the diffusion\nprocess, as illustrated in Fig. 1 (b). Specifically, we employ\nbeam search [86] tailored for intermediate steps and miti-\ngate the accumulated error at each step, while progressively\nselecting the golden initial noises by choosing the initial\nnoises. Moreover, rather than relying solely on the short-\nterm reward of locally noised clips, we predict the long-term\nconsequences of the initial noises to maintain high coher-\nence. A key challenge lies in the impracticality of directly\nassessing the initial noises, as it typically requires multiple\ndenoising steps to produce a clear image for assessment,\nresulting in an exponential increase in computational cost.\nTo address this, we introduce a one-step evaluation strategy\nthat employs the predicted clearer clip from the first DDIM\nstep as an efficient proxy of the quality of a fully denoised\nclip, as illustrated in Fig. 1 (c). Then, the 
predicted clip is\nfed into the reward model while preceding image serve as\nanchor, providing subject contextual information that pre-\nserves appearance consistency and supports long-term value\nestimation beyond the immediate search step.\nOur inte-\ngrated search strategy achieves a practical balance between\nglobal optimality, scalability, and efficiency, as shown in\nTable 1. Moreover, while greedy decoding easily becomes\ntrapped in local 
optima, the tree search strategy retains mul-\ntiple candidate sequences, thus exploring the search space\nmore comprehensively and enhancing both the quality and\ndiversity of the generated results.\nGiven limited computational resources, our strategy se-\nlects from a finite pool of candidate noises. Moreover, the\nquality of candidate noises constitutes a critical factor, com-\nplementing the robust reward model that provides long-term\nsupervisory signals.\nTo ensure the candidate pool com-\nprises noise that enhances video consistency, we construct\nit by sampling from a skewed distribution. Specifically, the\nweight of high-reward samples is increased, while samples\nare still drawn from the standard normal distribution to pre-\nserve diversity. Through this iterative search process, we\nsignificantly reduce accumulated errors and avoid inconsis-\ntencies arising from the randomness of initial noises.\nOn multiple benchmarks, we verify the effectiveness of\n2\nour method. Our main contributions are as follows:\n• We propose a plug-and-play inference-time scaling strat-\negy for long video generation by incorporating long-term\nsupervision signals into the reward process.\n• We introduce a one-step denoising approach that trans-\nforms the evaluation of initial noises into the evaluation\nof a clearer image without extra computational overhead.\n• Extensive experiments demonstrate that our proposed\nScalingNoise can be effectively applied to various video\ndiffusion models. It consistently improves the inference\nquality of generated videos.\n2. Related Work\nLong viedo generation. Video generation has advanced\nsignificantly [3, 59, 69, 95, 112, 113], yet producing high-\nquality long videos remains challenging due to the scarcity\nof such data and the high computational resources re-\nquired [3, 10, 77].\nThis limits training models for di-\nrect long video generation, leading to the widespread use\nof autoregressive approaches built on pre-trained mod-\nels [15, 91].\nCurrent solutions fall into two categories:\ntraining-based [38, 58, 76, 109] and training-free meth-\nods [5, 102, 107, 110]. Training-based methods [19, 84]\nlike NUWA-XL [96] use a divide-and-conquer strategy\nto generate long videos layer by layer, while FDM [22]\nand SEINE [12] combine frame interpolation and predic-\ntion. Other approaches, such as LDM [4], MCVD [72],\nLatent-Shift [2], and StreamingT2V [24], incorporate short-\nterm and long-term information as additional inputs. De-\nspite their success, these methods demand 
high-quality\nlong video data and substantial computation. Training-free\nmethods address these challenges. Gen-L-Video [74] and\nFreenoise [53] use a chunk-by-chunk approach, linking seg-\nments with final frames, but this risks degradation and in-\nconsistency. Freelong [42] blends global and local data via\nhigh-low frequency decoupling, while FreeInit [82] refines\ninitial noises for better consistency. FIFO-Diffusion [31]\nintroduces a novel paradigm, reorganizing denoising with\na noise-level queue, dequeuing clear frames, enqueuing\nnoise, for efficient, flexible long video generation with-\nout training.\nIn this work, we propose a plug-and-play\ninference-time strategy that can improve the consistency of\nvideos based on these long video generation methods.\nInference-time Search. A variety of inference-time search\nstrategies have been proven crucial in long context gener-\nation within the field of LLMs [29, 34, 40, 66, 92, 99].\nThe advent of DeepSeek-R1 [14] has further advanced\ninference-time search.\nBy applying various search tech-\nniques in the language space, such as controlled decod-\ning [7, 18, 90, 100], best of N [33, 36], and Monte\nCarlo tree search [67, 73, 79, 105], LLMs achieve better\nstep-level responses, thus enhancing performance. During\ninference-time search, leveraging a good process reward\nmodel (PRM) [13, 27, 70, 78] is essential to determine the\nquality of the responses. Ma et al. [45] proposed using su-\npervised verifiers as a signal to guide generating trajectory\nwithin Diffusion Models (DMs), but did not investigate its\nimpact on video generation inference-time search. Further-\nmore, some work has preliminarily explored inference-time\nsearch in VDMs [68, 69, 85, 104], however, there is still\na lack of investigation into 
the inference-time scaling law\nand long-term signals in the process of long video genera-\ntion. In this work, we explore the effectiveness of scaling\ninference-time budget utilizing beam search to enhance the\nconsistency of generated long videos.\n3. Methodology\n3.1. Preliminaries: Video Denoising Approach\nLong Video Generation. We introduce the formulation of\nVDM for generating long videos. There are two approaches\nto create a long video: the chunk-by-chunk method and the\nFIFO diffusion method. Both paradigms can be seamlessly\nintegrated into the subsequent framework. Consider extend-\ning a pre-trained model that generates fixed-length videos\ninto a long video generation model with a distribution of pθ,\nbased on the two approaches mentioned above. This model\nprocesses an input consisting of both prompt and image to\ngenerate a video v = [v1, v2, . . . , vN], where v consists\nof N step-level responses. Each step-level response vi is a\nframe or video clip of the long video, treated as a sample\ndrawn from a conditional 
probability distribution:\nvi = pθ(vi|z<i, c),\ni = 1, 2, . . . , T,\n(1)\nwhere c can be denoted as a single prompt or a prompt,\nimage pair and z<i 
= [v1, v2, . . . , vi] is the concatenated\nvideo. In the following, we provide the specific step-level\nformulations for these two paradigms, respectively:\n• Chunk by Chunk:\nChunk-by-chunk is a generation\nparadigm [47, 74, 103] that operates through a sliding win-\ndow approach, using the last few frames generated in the\nprevious chunk as the starting point for the next chunk\nto continuously produce content. In this paradigm, vi =\n{vf\ni }M\nf=1 denotes a video clip of a fixed length M produced\nby the generated model, while vi,t denotes t noise level of\nthe video clip. A chunk-by-chunk step can be formalized\nas:\nvi,t−1 = Ψ (vi,t, t, ϵθ(vi,t, t, c)) ,\n(2)\nwhere Ψ and ϵθ denote a sampler such as DDIM and a noise\npredict network, respectively.\n• FIFO-Diffusion:\nDifferent from the aforementioned\nprocess, FIFO-Diffusion [31] introduces a diagonal denois-\ning paradigm [9, 56] by rescheduling noise steps during the\ninference process. FIFO-Diffusion achieves autoregressive\ngeneration by maintaining a queue where the noise 
level in-\ncreases step by step. We define the queue as Q = {vi,t}T\nt=1,\n3\nwhere t denotes the noise level, and i indicates the i-th\nframe in the 
queue. In this paradigm, i is equal to t, and\nvi denotes a frame. The length of T is M × P where P\ndenotes the partition of the queue. The procedure of FIFO\nstep can be described as follows:\nQ = Ψ\n\x10\nQ, {t}T\nt=1 , ϵθ(Q, {t}T\nt=1 , c)\n\x11\n.\n(3)\nDDIM. DDIM [63] introduces a new sampling paradigm by\nde-Markovizing the process, which remaps the time steps\nof DDPM [25] from [0; . . . ; T] to [τ0; . . . ; τT ], which is a\nsubset of the initial diffusion scheduler, thereby accelerating\nthe sampling process of DDPM. Here, DDIM(vτt) consists\nof three distinct components, which can be formulated as:\nvτt−1 = DDIM(vτt)\n= ατt−1\n\x12vτt −στtϵθ(vτt, τt)\nατt\n\x13\n+ στt−1ϵθ(vτt, τt),\n(4)\nwhere α and στt denote predefined schedules.\n3.2. Formulation of Video Generation Inference\nThe long video generation framework can be formulated as\na Markov Decision Process (MDP) problem 
defined by the\ntuple (S, A, R). S is the state space. Each state is defined\nas a combination of the generated video and the condition.\nThe initial 
state s0 only corresponds to the input prompt\nx and guided image I. Si is the combination of the cur-\nrently generated videos. A denotes the action 
space, where\neach action encompasses a two-part process: sampling an\ninitial noise from a tilted distribution, followed by denois-\ning the current 
video clip based on the noise. We also have\nthe reward function R to evaluate the reward of each action,\nwhich is also known as the process reward model (PRM) in\nLLMs. With this MDP modeling, we can search for ad-\nditional states by increasing the inference-time compute,\nthereby obtaining a better VDM response v. Specifically,\nwe can take different actions in each state, continuously ex-\nplore, and then make choices based on the reward model to\nachieve a better state. The general formulation of the selec-\ntion process is:\nat+1 = arg max\nA Φ(st, A),\n(5)\nwhere Φ denotes the reward model R : S × A →R. The\ncore of our method focuses on efficiently and accurately es-\ntimating then selecting initial noises, improving video gen-\neration with better guidance.\n3.3. Reward Design\nDuring the search process, our objective is to evaluate the\nconsistency and quality of the video at 
each step, using these\nAlgorithm 1 ScalingNoise Inference-time Search\nRequire: Diffusion Model D, Reward Function Φ, Sample\nTilted Distribution Sample, Condition c, Beam Size k,\nStep Size n, DDIM Steps τt, Generated Video V = [ ]\nAnchor Frame va\n1: while Generation is not Done do\n2:\nfor i in [1, 2, ..., k] do\n3:\nr = [ ]\n4:\nfor j in [1, 2, ..., n] do\n5:\nϵij ←Sample(V )\n6:\nˆvij ←D(ϵij, c, num steps = τt)\n7:\nrij ←Φ(ˆvij, va)\n8:\nr.append(rij)\n9:\nend for\n10:\nend for\n11:\n[v1, . . . , vk] ←Select the best k elements from r\n12:\nfor i in [τ0, τ1, ..., τt] do\n13:\nv ←D(ϵ, c, num steps = i)\n14:\nend for\n15:\nva ←vi0\n16:\nAppend current clip [v1, . . . , vk] to V\n17: end while\n18: return V\ninsights to guide subsequent searches, as illustrated in Fig. 1\n(c). The evaluation of each search step, defined as apply-\ning an action at to the state st, is performed by a reward\nfunction rt = Φ(st, at) ∈R. Below, we elaborate on the\nspecific design of this reward function.\nOne-Step Denoising. Throughout our evaluation process,\nthe actions in the MDP sample initial noises, which is typ-\nically Gaussian noise. Therefore, it is difficult to assess.\nA key challenge 
arises from the difficulty of directly as-\nsessing this initial noises. To address this, we propose a\none-step evaluation approach. Specifically, we utilize the\npredicted ˆvτ0 component from Eq. 4 above as the target for\nevaluation. The detailed formulation is presented as:\nˆvτ0 = vτt −στtϵθ(vτt, τt)\nατt\n.\n(6)\nOur evaluation method leverages a single DDIM step to ac-\ncurately and efficiently assess the initial noise. In contrast\nto the original brute-force approach, which fully denoises\nthe initial noises into a clear video, our technique is less\ncomputationally intensive, though 
it may yield suboptimal\nresults. The brute-force method, while thorough, requires\nsignificant computational resources, rendering it impracti-\ncal for scaling to long video generation.\nConsistency Reward.\nAfter obtaining an evaluable ob-\nject, we need to design a long-term reward to evaluate the\noverall consistency of the video and prevent the accumula-\ntion of errors. The reward function requires careful design.\n4\nSpecifically, we take 
into account not only the video clip\ncurrently being generated by the action but also the states of\nprevious nodes. To this end, we select fully denoised, clear\nvideo frames va from several frames prior as anchor points\nand assess the consistency of the video within the current\nwindow after one-step denoising. This approach substan-\ntially reduces inconsistencies in video generation. In our\nexperiments, we employ a DINO [6] model and calculate\nthe reward using the following formula:\nΦ :=\n1\n2(T −1)\nM\nX\ni=1\n(⟨da · di⟩+ ⟨di · di−1⟩) ,\n(7)\nwhere da and di denote the image features of the anchor\nframe and the i-th frame in the current clip, respectively.\nAnd ⟨·⟩is the dot product operation for calculating cosine\nsimilarity.\n3.4. Action Design\nSearch Framework. Once equipped with a rewards model\n(Section 3.3), VDM can leverage any planning algorithm\nfor search, as demonstrated in\n[21].\nWe employ beam\nsearch (Fig. 1 (b)), a robust planning technique that effi-\nciently navigates the search tree space, effectively balancing\nexploration and exploitation to identify high-reward trajec-\ntories. Specifically, beam search constructs a search tree it-\neratively, with each node representing a state and each edge\ndenoting an action and the resulting state transition. To steer\nVDM toward the most promising nodes, 
the algorithm relies\non a state-action reward function, Φ in Eq. 7. To promote\ndiversity among response candidates at each step, we main-\ntain K distinct trajectories. From a tilted distribution, we\nsample N initial noise instances, generating K × N candi-\ndates for the current step. The reward model evaluates each\ncandidate, and the top-K candidates with the highest scores\nare selected as responses for that step. This sampling and\nselection process iterates until the full response sequence\nis generated. Further details and the pseudo-code for this\nplanning algorithm are provided in 
Algorithm 1.\nTilted Distribution Sampling. During the search process,\nwe need to sample the initial noises. Due to computational\nconstraints, exhaustively searching the entire noise space is\ninfeasible. To increase the likelihood of generating superior\nresults, we sample initial noise from a tilted distribution [52,\n60]. Specifically, we construct a high-quality candidate pool\nby employing four distinct tilted distributions to sample the\ninitial noises. These operations are detailed as follows:\n⋄A1 Random Sampling: Directly sample noise from a\nGaussian distribution vi ∼N(0, I).\n⋄A2 
FFT Sampling: Utilize 2D/3D Fast Fourier Trans-\nform (FFT) [11, 42] to blend Gaussian noise with the last\nfew frames, denoted as:\nvi = F r\nlow(vi) + F r\nhigh(η) ,\n(8)\nwhere F denotes the FFT function, and η is the Gaussian\nnoise.\n⋄A3 DDIM Inversion: Apply DDIM Inversion to re-\nnoise the previous frame. It can be formulated as:\nvt−1 = αt−1\n\x12vt −σtϵθ(vt, t)\nαt\n\x13\n+ σt−1ϵθ(vt, t).\n(9)\n⋄A4 Inversion Resampling: Building on DDIM 
Inver-\nsion, sample new noise within its δ-neighborhood defined as\n{v′ : d(v, v′) < δ}. Through these strategies, we enhance\nthe quality of candidate noise, enabling us, within limited\nresources, to maximize the leveraging of actions with higher\nrewards.\n4. Experiment\nIn this section, we conduct experiments to answer the fol-\nlowing two questions: 1. Does the long-term reward guided\nsearch yield higher-quality and consistent video compared\nwith other inference-time scaling methods? 2. Does the\none-step evaluation provide an efficient and accurate as-\nsessment of initial noises?\n4.1. Baseline and Implementation details\nTo evaluate the effectiveness and generalization capacity of\nour proposed method, we implement it on the text-to-video\nFIFO-Diffusion [31] and image-to-video chunk by chunk\nlong video generation, based on existing open-source dif-\nfusion models trained on 
short video clips. These models\nare limited to producing videos with a fixed length of 16\nframes. In our experiments, the evaluations are conducted\non an NVIDIA A100 GPU.\nVbench Dataset.\nOur approach is systematically evalu-\nated using VBench [28], a video generation benchmark\nfeaturing 16 metrics crafted to thoroughly evaluate mo-\ntion quality and semantic consistency. We select 40 rep-\nresentative prompts spanning all categories, generate 100\nvideo frames, and analyze the model’s performance accord-\ningly. We choose five VBench evaluation metrics for per-\nformance comparison: Subject Consistency, Background\nConsistency, Motion Smoothness, Temporal Flickering, and\nImaging Quality. We maintain k distinct candidates in the\nbeam and sample n completions for each one. Specifically,\nwe set k = 2, 3 for beam size, and n = 5 with FIFO-Diffusion\nbased on VideoCraft2 [10], n = 10 with ConsistI2V [55]\nchunk by chunk to balance the quality and efficiency. For\nthe I2V model, we use the first image of the video generated\nby FIFO-Diffusion to guide the video generation. We con-\nsider two types of baselines: (1) Base Model: This serves\nas our baseline approach, employing a 
naive method that\ndeliberately avoids any form of inference-time scaling. (2)\nBest of N (BoN): A widely adopted technique to improve\nmodel response quality during inference. For each prompt,\n5\nMethod\nNcan\nSubjection\nConsistency↑\nBackground\nConsistency↑\nMotion\nSmoothing↑\nTime\nFlicking↑\nImaging\nQuality↑\nOverall\nScore↑\nStreamingt2v [24]\n1\n84.03\n91.01\n96.58\n95.47\n61.64\n85.74\nOpenSora v1.1 [32]\n1\n86.92\n93.18\n97.50\n98.72\n53.07\n85.87\nFreeNoise [53]\n1\n92.30\n95.16\n96.32\n94.94\n67.14\n89.17\nConsistI2V(Chunk-wise) [55]\n1\n89.57 ↓+0.00\n93.22 ↓+0.00\n97.62 ↓+0.00\n96.63 ↑+0.00\n55.21 ↓+0.00\n86.45 ↓+0.00\n+BoN\n3\n89.92 ↑+0.35\n93.64 ↑+0.42\n97.59 ↓−0.03\n96.66 ↑+0.03\n55.74 ↑+0.50\n86.71 ↑+0.26\n+BoN\n5\n90.56 ↑+0.99\n93.59 ↑+0.37\n97.73 ↑+0.11\n96.24 ↓−0.39\n56.24 ↑+1.03\n86.87 ↑+0.42\n+ScalingNoise (Ours)\n2\n91.58 ↑+2.01\n94.36 ↑+1.14\n97.85 ↑+0.25\n96.79 ↑+0.16\n56.82 ↑+1.61\n87.48 ↑+1.03\n+ScalingNoise (Ours)\n3\n92.02 ↑+2.45\n94.44 ↑+1.22\n97.91 ↑+0.29\n96.97 ↑+0.34\n58.12 ↑+2.91\n87.89 ↑+1.18\nFIFO-Diffusion [31]\n1\n90.26 ↓+0.00\n93.53 ↓+0.00\n95.86 ↓+0.00\n92.78 ↓+0.00\n65.52 ↓+0.00\n87.59 ↓+0.00\n+BoN\n3\n90.92 ↑+0.66\n94.49 ↑+0.96\n95.20 ↓−0.66\n93.76 ↑+0.98\n64.13 ↓−1.39\n87.70 ↑+0.11\n+BoN\n5\n91.26 ↑+1.00\n94.91 ↑+1.38\n95.97 ↑+0.11\n93.97 ↑+1.19\n64.37 ↓−1.15\n88.10 ↑+0.51\n+ScalingNoise (Ours)\n2\n91.60 ↑+1.34\n93.74 ↑+0.21\n96.67 ↑+0.81\n94.96 ↑+2.18\n65.67 ↑+0.15\n88.53 ↑+0.94\n+ScalingNoise (Ours)\n3\n93.14 ↑+2.88\n94.61 ↑+1.08\n97.01 ↑+1.15\n95.34 ↑+2.56\n67.91 ↑+2.39\n89.60 ↑+2.01\nTable 2. Quantitative comparison results. Comparison of performance metrics for various video generation methods as benchmarked\nby VBench. We calculate the average performance in the last column, demonstrating its effectiveness in producing fidelity and consistent\nlong videos. Bold indicates the highest value, and underlined indicates the second highest.\nwe generate multiple responses, specifically 3 and 5 dis-\ntinct outputs. We also select three state-of-the-art meth-\nods as baselines, namely StreamingT2V [24], Openasora\nv1.1 [32], and FreeNoise [53].\nUCF-101 Dataset. UCF-101 [64] is a large-scale human\naction dataset containing 13,320 videos across 101 action\nclasses. We 
utilize the following metrics:\n• Frechet Video Distance (FVD) [71] for temporal coher-\nence and motion realism.\n• Inception Score (IS) [57] for frame-level quality and di-\nversity.\nWe measure the two metrics using Latte [46] as a base\nmodel, which is a DiT-based video model trained on UCF-\n101 [64], employing FIFO-Diffusion as the paradigm for\nlong video generation, configured with k = 2 and m = 5.\nWe generate 2,048 videos with 128 frames each to calcu-\nlate FVD128, a version of FVD which uses 128-frames-long\nvideos to compute the statistics, and randomly sample a 16-\nframe clip from each video to measure the IS score, fol-\nlowing evaluation guidelines in StyleGAN-V [61]. As the\nbase model, we choose StyleGAN-V, PVDM [101], FIFO-\nDiffusion, as they are representative open-sourced models.\n4.2. Quantitative results\nScale Search Improves Video Consistency. We compare\nScalingNoise with the baselines trained for long video gen-\neration in terms of multiple benchmarks.\nVbench: As\nshown in Table 2, we find that the videos generated by Scal-\ningNoise are significantly more preferred compared with\nthe baseline. Videos generated by the base model decod-\ning are of the lowest quality. While increasing inference\ncompute via BoN shows improvement, they still fall short\ncompared with ScalingNoise search.\nThe computational\nIS(↑)\nFVD(↓)\n80\n60\n40\n20\n  0\n300\n1800\n1500\n1200\n900\n600\n  0\nStyleGAN-V\nFIFO-Diffusion\nPVDM\nScalingNoise\n1773.4\n648.4\n596.6\n539.2\n23.94\n74.40\n74.44\n74.95\nFigure 2. Comparisons of FVD128 and IS scores on UCF-101.\nScalingNoise utilizes Latte [46] as its baseline, where the number\nof beam sizes is 2, and noise candidates are 5. The FVD and IS\nscores of the other algorithms are obtained from their respective\npapers, and PVDM [101] denotes PVDM-L (400-400s).\ncost of BoN(3) is comparable to that of our method when\nconfigured with k = 3. In contrast, the computational cost\nof BoN(5) significantly exceeds that of our approach. Al-\nthough its consistency has improved, our method outper-\nforms BoN(5) across all evaluated aspects. The long videos\nobtained using ScalingNoise search not only significantly\nehnance consistency, but also provide high quality video\nframes. FVD and IS: As illustrated in Fig. 2, our approach\noutperforms all the compared methods including PVDM-L\n(400-400s) [101], which employs a chunked autoregressive\ngeneration strategy. Note that PVDM-L (400-400s) itera-\ntively generates 16 frames conditioned on the previous out-\nputs over 400 diffusion steps.\nBenefits from Further Scaling Up Inference Compute.\nWe next explore the effect of increasing inference-time\ncomputation on the response quality of the VDM at each\n6\n1\n3\n4\n2\nBeam Size\n 90\n95\n100\n 80\n 85\n 75\nSubject Consistency\n0.800\n0.850\n0.950\n0.875\n0.925\n0.900\n0.825\n0.975\n1.000\n0.955\n0.970\n0.975\n0.980\n 0.985\n0.960\n0.965\n0.84\n0.90\n0.92\n0.94\n0.96\n0.86\n0.88\n0.800\n0.850\n0.950\n0.875\n0.925\n0.900\n0.825\n0.975\n1.000\n1\n3\n4\n2\nBeam Size\n 90\n95\n100\n 80\n 85\n 75\nSubject Consistency\nDINO Score\nCLIP Score\n(a)  Scaling tendency of search beam size. \n(b) Correction of reward score and consistency.\nSubject Consistency\nSubject Consistency\nr = 0.2376\nr = 0.0149\nFigure 3. (a) The two figures are boxplots showing the tendency of scaling beam sizes for ScalingNoise based on two paradigms, in order\nof FIFO-Diffusion and Chunk by chunk. (b) From Left to Right: Correction of reward model DINO and CLIP feature similarity score and\nfinal subject consistency. All points are generated by VideoCraft2.\nMethod\nSubjection\nConsistency↑\nOverall\nScore↑\nInference\nTime↓\nBoN\n97.87\n92.06\n477.75\n10-Step\n97.14\n91.59\n79.67\nScalingNoise (Ours)\n97.71\n91.83\n12.34\nTable 3. Video consistency and inference times of different eval-\nuation methods. ScalingNoise utilizes one-step evaluation to sig-\nnificantly improve efficiency.\nstep by varying the beam sizes. To ensure experimental\nfairness, we set n = 5 for FIFO-Diffusion, while for chunk-\nby-chunk methods, we maintain n = 10. This distinction\narises because FIFO-Diffusion requires initializing only one\ninitial noise per step, whereas the chunk-wise search pro-\ncess initializes one clip per step. Consequently, after a sin-\ngle DDIM denoising step, the chunk-wise approach needs\nto select the combination of multiple frames, resulting in\na search space complexity significantly larger than that of\nFIFO-Diffusion. Setting n = 5 would yield an overly con-\nstrained search space and fail to align with the computa-\ntional load of FIFO-Diffusion. We adopt the subject con-\nsistency from V-Bench, as mentioned earlier, as our eval-\nuation criteria. We report the scores for long video gener-\nation achieved through ScalingNoise search, based on both\nFIFO-Diffusion and chunk-by-chunk paradigms, with beam\nsizes set to 1, 2, 3, and 4. The experimental results are illus-\ntrated in Fig. 3 (a). Since some prompts are static while oth-\ners correspond to video actions with very large movements,\nthis results in a significant variance. Our observations re-\nveal that the performance of ScalingNoise, for both FIFO-\nDiffusion and chunk-wise strategies, improves steadily as\nthe search beam size increases. This trend suggests that\nscaling inference-time computation effectively enhances the\nvisual consistency capabilities of VDM.\nOne-Step Evaluation’s Efficiency and Accuracy.\nTo\nevaluate computational efficiency and accuracy of our eval-\nuation method, we examine the VBench evaluation met-\nrics and inference time across different assessment meth-\nMethod\nSubjection\nConsistency↑\nImage\nQuality↑\nOverall\nScore↑\nLocal\n92.16\n66.83\n88.94\nAnchor\n92.67\n66.39\n89.28\nScalingNoise (Ours)\n93.14\n67.91\n89.60\nTable 4. Reward Function Studies. Video consistency and qual-\nity of different reward function guided inference time search.\nods. Using the VideoCraft2 [10] model (configured with\nDDIM=64), we generate videos with a fixed length, adopt-\ning 16 frames. We sample 10 candidate initial noises and\nemploy our one-step evaluation method to select one. To\nensure a fair comparison of initial noise quality, we fully\ndenoise the selected noise and assess the consistency and\nquality of the resulting video. In addition, we test two base-\nline approaches: (1) BoN: selection after complete denois-\ning for clarity, and (2) m-Step Evaluation: the initial noises\nare denoised for m steps via DDIM, followed by selection\nusing the same reward model. Here we set m to 10. As\nshown in Table 3, our one-step evaluation method gener-\nates videos in just 12.34 seconds, enabling the assessment\nand selection of initial noises without compromising base-\nline performance. In contrast, the other two baselines re-\nquire 79.67 seconds and 477.75 seconds, respectively. This\nefficiency allows for scalable search within the recent long\nvideo generation paradigm.\n4.3. Qualitative results\nUser Study. We start with human evaluation with results\nshown in Fig. 4. We utilize generated videos from the eval-\nuation dataset, allowing human annotators to assess and\ncompare the output quality and consistency across differ-\nent methods.\nThe win rate is then calculated based on\ntheir judgments, providing a clear metric for performance\ncomparison. The robust performance of our method, Scal-\ningNoise, underscores its capability to produce videos that\nare not only more natural and visually coherent but also\nmaintain a high level of consistency throughout. Compared\n7\nScalingNoise\nvs\nFIFO-BoN3\nScalingNoise\nvs\nFIFO-BoN5\nScalingNoise\nvs\nChunk-BoN3\nScalingNoise\nvs\nChunk-BoN5\nScalingNoise Wins\nScalingNoise Loses\nTie\n0\n20\n80\n100\n40\n60\nPersentage(%)\nFigure 4. User Study. Win rate of videos generated using Scal-\ningNoise compared with other inference-time scaling methods.\nto the naive inference time scaling method, BoN, Scaling-\nNoise distinctly showcases its superior efficiency.\nCase Study of Search Trajectory. As shown in Fig. 5,\nScalingNoise demonstrates a clear search process based on\nconsistI2V, which illustrates how it evolves from starting\nstate (contains a prompt and a guided image) into a com-\nplete long video. In each step, ScalingNoise employs a se-\nlection, steering by the long-term reward function. For 
ex-\nample, in the first step, the reward model assigns a higher\nscore to images where red wine is not spilled, thus avoiding\nsubsequent cumulative 
errors. At the same time, it can be\nseen that due to our long-term reward strategy, even if a bad\ncase has already occurred, our method can still make cor-\nrections to subsequent frames based on the anchor frame.\n4.4. Ablation Study\nIn this section, we conduct ablation studies to evaluate the\nimpact of each design component in ScalingNoise for long\nvideo generation. Unless otherwise specified, all experi-\nments follow previous settings for fair comparison.\nLong-Term Reward. First, we present the performance of\ndifferent variants explored in the design of the reward func-\ntion based 
on FIFO. Table 4 details the results of two addi-\ntional runs, while still using the DINO model, with differ-\nent reward functions: (1) local reward: using only local clip\nduring the denoising process, and (2) anchor reward: using\nonly the initial noise and anchor frame, leading to a drop\nof 0.66% and 0.32%, respectively. The specific calculation\nformula is as follows:\nΦlocal =\nX\n< di · di−1 >,\nΦanchor =< da · dn > .\nAs shown in Table 
4, ScalingNoise achieves the best perfor-\nConsistI2V\n+ScalingNoise\nv=0.86\nv=0.94\nv=0.93\nv=0.92\nv=0.93\nv=0.93\nI=1\n......\nAnchor\nMemory Update\nFigure 5.\nThe upper part of this figure represents the original\nmethod, which involves a greedy approach to generating long\nvideos. In contrast, the tree-structured searching process of Scal-\ningNoise is outlined below: We highlight the path from the ini-\ntial state to the best-performing generation, reporting the score for\neach node. Our prompt is “Red wine is poured into a glass. highly\ndetailed, cinematic, arc shot, high contrast, soft lighting”.\nMethod\nSubjection\nConsistency↑\nImage\nQuality↑\nOverall\nScore↑\nRandom\n92.64 ↓+0.00\n65.98 ↓+0.00\n89.07 ↓+0.00\n2D FFT\n92.67 ↑+0.03\n65.79 ↓−0.19\n89.24 ↑+0.17\nResample\n92.94 ↑+0.30\n65.71 ↓−0.27\n88.83 ↓−0.11\nReverse\n93.27 ↑+0.63\n64.83 ↓−1.15\n88.96 ↓−0.66\nAll (Ours)\n93.14 ↑+0.50\n67.91 ↑+1.93\n89.60 ↑+0.53\nTable 5. Tilted Distribution studies. Comparsion of sampling\nfrom the different tilted distribution.\nmance. As illustrated in Fig. 6, we present videos guided by\ndifferent reward models. Our reward function not only con-\nsiders the long-term consistency between the initial noise\nand anchor frame, but also effectively accounts for the\ncross-temporal influence of initial noise propagation across\nvideo frames within the denoising window.\nDifferent Reward Model. Then we investigated the ef-\nfects of using different reward models (i.e., DINO [6] and\nCLIP [54]) to guide the search process. For each reward\nmodel, we generated a 16-frame video and, after one de-\nnoising step, scored it using DINO and CLIP, respectively.\nAs shown in Fig. 3 (b), the vertical axis still represents\nthe subject consistency score, while the horizontal axis rep-\nresents the reward model scores. It can be observed that\nDINO’s similarity scores demonstrate a stronger alignment\nwith the final video’s subject consistency compared to CLIP.\nSpecifically, there exists a robust correlation between the\nconsistency of the main subject in the final video and the\nfeature space extracted by DINO, as measured by the co-\nsine similarity of the video frames. In contrast to DINO,\n8\nAnchor\nLocal \nScalingNoise \n"An astronaut walking on the moon\'s surface, high-quality, 4K resolution."\nFigure 6. Example Videos. Illustrations of long videos guided by different reward function. Row 1: Local reward only consider the quality\nof current denoised clip. Row 2: Anchor reward calculate the similarity of the anchor frame and the initial noise. Row 3: Ours combines\nthe best of both, achieving long-term reward.\nwhich effectively captures the features of the primary sub-\nject in each frame, CLIP tends to focus on extracting the\noverall features of the background. During video genera-\ntion, inconsistencies predominantly stem from variations in\nthe subject, while changes in the background remain rela-\ntively minor. Consequently, DINO provides a more accu-\nrate and reliable evaluation of subject consistency, making\nit a superior choice over CLIP for this purpose.\nEffectiveness of Tilted Distribution. We investigate how\neach component of the tilted distribution impacts the quality\nof video generation. Table 5 summarizes the performance\nresults 
for long video generation.\nWe tested the perfor-\nmance of these sampling distributions separately, including\n(1) Random Distribution, the standard 
normal Gaussian dis-\ntribution. (2) 2D FFT, which leverages frequency domain\nanalysis to improve generation quality. (3) DDIM Inver-\nsion, a method that significantly enhances subject consis-\ntency across frames. (4) Inversion Resampling, an approach\nintroduced to address specific limitations observed in earlier\nmethods. The 2D FFT is an effective method for improving\nvideo quality. However, as the generated length increases,\nit can lead 
to a degradation in video quality. Although the\nDDIM reverse markedly enhances the subject consistency,\nit results in a significant reduction in the range of motion in\nthe generated video. Therefore, we introduce Inversion Re-\nsampling to maintain the diversity. Integrating all methods\ninto the base model yields a performance boost of +0.53%.\n5. Conclusion & Limitation\nIn this work, we introduce ScalingNoise, a novel inference-\ntime search strategy that significantly enhances the consis-\ntency of VDMs by identifying golden initial noises to opti-\nmize video generation. Utilizing a guided one-step denois-\ning process and a reward model anchored to prior content,\nScalingNoise achieves superior global content coherence\nwhile maintaining high-level object features across multi-\nchunk video sequences. Furthermore, by integrating a tilted\nnoise distribution, it facilitates more 
effective exploration\nof the state space, further elevating generation quality. Our\nfindings show that scaling inference-time computations en-\nhances both video consistency and the quality of individ-\nual frames. Experiments on benchmarks validate that Scal-\ningNoise substantially enhances content fidelity and subject\nconsistency in resource-constrained long video generation.\nLimitation: We clarify the limitations of our proposed\nScalingNoise: (i): Our ScalingNoise may struggle with\nscenes involving highly complex or abrupt motion, where\naccurate alignment across frames becomes challenging,\npotentially affecting temporal coherence.\n(ii):\nScal-\ningNoise cannot completely eliminate accumulated error,\nwhile through long-term signal guidance, it can to some\nextent alleviate this phenomenon. To entirely address this\nissue, we need to conduct an extra in-depth analysis of the\ncauses of error accumulation.\n9\n6. VideoCrafter2\nIn Fig. 7 and Fig. 8, we provide more qualitative results with VideoCrafter2 [10].\n(a) ”A lone figure in a hooded cloak stands atop a skyscraper, city lights sprawling below like glowing circuit board.”\n(b) ”A mystical, low-poly enchanted garden with glowing plants and a small pond where fireflies dance under a starry sky.”\n(c) ”A pair of tango dancers performing in Buenos Aires, 4K, high resolution.”\n(d) ”A tiger walks in the forest, photorealistic, 4k, high definition, 4k resolution.”\n(e) ”A tranquil, low-poly mountain village at dawn, with smoke rising from chimneys and birds flying over snowy peaks.”\n(f) ”Cinematic closeup and detailed portrait of a reindeer in a snowy forest at sunset.”\nFigure 7. Videos generated by ScalingNoise with VideoCrafter2 based on the paradigm of FIFO-Diffusion.\n10\n(a) ”A dynamic, low-poly cityscape at night, with neon lights reflecting off wet streets and a lone cyclist riding through the rain.”\n(b) ”A bustling cityscape at sunset with skyscrapers reflecting golden light, people walking, and traffic moving swiftly.”\n(c) ”A peaceful, low-poly countryside with rolling hills, a windmill, and a farmer tending to his crops under a golden sunset.”\n(d) ”A horse race in full gallop, capturing the speed and excitement, 2K, photorealistic.”\n(e) ”A cozy, low-poly cabin in the woods surrounded by tall pine trees, with a warm light glowing from\nthe windows and smoke curling from the chimney, 4k resolution.”\n(f) ”Impressionist style, a yellow rubber duck floating on the wave on the sunset, 4k resolution.”\nFigure 8. Videos generated by ScalingNoise with VideoCrafter2 based on the paradigm of FIFO-Diffusion.\n11\nIn Fig. 9, we present the last individual frame of the generated videos.\nFigure 9. The last individual video frame generated by ScalingNoise with VideoCrafter2 based on the paradigm of FIFO-Diffusion.\n12\nReferences\n[1] Learning to reason with llms. https://openai.com/\nindex / learning - to - reason - with - llms/,\n2024. 2\n[2] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-\nBin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent\ndiffusion with temporal shift for efficient text-to-video gen-\neration, 2023. 3\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023. 3\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim\nDockhorn, Seung Wook Kim, Sanja Fidler, and Karsten\nKreis. Align your latents: High-resolution video synthe-\nsis with latent diffusion models.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 22563–22575, 
2023. 2, 3\n[5] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu,\nZhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu\nYue. Ditctrl: Exploring attention control in multi-modal\ndiffusion transformer for tuning-free multi-prompt longer\nvideo generation, 2024. 3\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 9650–9660, 2021. 5, 8\n[7] Souradip Chakraborty, Soumya Suvra Ghosal, 
Ming Yin,\nDinesh Manocha, Mengdi Wang, Amrit Singh Bedi, and\nFurong Huang. Transfer q star: Principled decoding for llm\nalignment. arXiv preprint arXiv:2405.20495, 2024. 3\n[8] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or. Attend-and-excite: Attention-based se-\nmantic 
guidance for text-to-image diffusion models, 2023.\n2\n[9] Boyuan Chen, Diego Mart´ı Mons´o, Yilun Du, Max Sim-\nchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion\nforcing: Next-token prediction meets full-sequence diffu-\nsion. In Advances in Neural Information Processing Sys-\ntems, pages 24081–24125. Curran Associates, Inc., 2024.\n3\n[10] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia,\nXintao Wang, Chao Weng, and Ying Shan. Videocrafter2:\nOvercoming data limitations for high-quality video diffu-\nsion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7310–\n7320, 2024. 3, 5, 7, 10\n[11] Jingyuan Chen, Fuchen Long, Jie An, Zhaofan Qiu, Ting\nYao, Jiebo Luo, and Tao Mei. Ouroboros-diffusion: Explor-\ning consistent content generation in tuning-free long video\ndiffusion. arXiv preprint arXiv:2501.09019, 2025. 2, 5\n[12] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu.\nSeine: Short-to-long video 
diffu-\nsion model for generative transition and prediction. arXiv\npreprint arXiv:2310.20700, 2023. 3\n[13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\nTworek, Jacob Hilton, Reiichiro Nakano, et al.\nTrain-\ning verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021. 3\n[14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai\nYu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhu-\noshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang,\nBochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai,\nDeli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong\nDai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang,\nHonghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li,\nJianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen,\nJingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni,\nJian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang\nGuan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang,\nLiang Zhao, Litong 
Wang, Liyue Zhang, Lei Xu, Leyi\nXia, Mingchuan Zhang, Minghua Zhang, Minghui Tang,\nMeng Li, Miaojun Wang, Mingming Li, Ning Tian, Pan-\npan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen,\nQiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji\nWang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu,\nShangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu\nWang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun,\nTian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia\nZhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,\nWentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xi-\naohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng,\nXin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li,\nXuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiao-\njin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang,\nXinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan,\nY. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong\nXu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\nYu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He,\nYishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan\nLiu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong,\nYuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yux-\niang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong\nXu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu,\nYunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z.\nRen, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda\nXie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhi-\ngang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin\nLi, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang,\nZhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-\nr1: Incentivizing reasoning capability in llms via reinforce-\nment learning, 2025. 3\n[15] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo,\nYufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi,\nand Xinlong Wang. Autoregressive video generation with-\n13\nout vector quantization, 2025. 3\n[16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. Structure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346–7356, 2023. 2\n[17] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu\nYang, and Di Huang. Initno: Boosting text-to-image diffu-\nsion models via initial noise optimization, 2024. 2\n[18] Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and\nBin Hu. Cold-attack: Jailbreaking llms with stealthiness\nand controllability, 2024. 3\n[19] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie\nLin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long con-\ntext tuning for video generation, 2025. 3\n[20] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel\nShalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy\nShiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weiss-\nbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and\nOfir Bibi. Ltx-video: Realtime video latent diffusion, 2024.\n2\n[21] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with\nlanguage model is planning with world model, 2023. 5\n[22] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-\ntian Weilbach, and Frank Wood. Flexible diffusion model-\ning of long videos. In NeurIPS, 2022. 3\n[23] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,\nand Qifeng Chen.\nLatent video diffusion models for\nhigh-fidelity long video generation.\narXiv preprint\narXiv:2211.13221, 2022. 2\n[24] Roberto\nHenschel,\nLevon\nKhachatryan,\nDaniil\nHayrapetyan,\nHayk Poghosyan,\nVahram Tadevosyan,\nZhangyang Wang, Shant Navasardyan, and Humphrey\nShi.\nStreamingt2v:\nConsistent, dynamic, and extend-\nable long video generation from text.\narXiv preprint\narXiv:2403.14773, 2024. 3, 6\n[25] Jonathan Ho, Ajay Jain, and Pieter